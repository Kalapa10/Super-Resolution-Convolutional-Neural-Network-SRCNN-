# -*- coding: utf-8 -*-
"""Original

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9xVWctf3OcBeM8EPwfm6FBR6rdPE2P1
"""

#Pakai GPU
import tensorflow as tf
tf.test.gpu_device_name()

#Listing device
from tensorflow.python.client import device_lib
device_lib.list_local_devices()

#Checking RAM
!cat /proc/meminfo

import numpy as np
import cv2
import glob
import tensorflow as tf
from tensorflow.keras import Model, Input, regularizers
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.preprocessing import image
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split 
import pickle

import glob

# Importing drive method from colab for accessing google drive
from google.colab import drive

# Mounting drive
# This will require authentication : Follow the steps as guided
from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive/TA SRCNN/Data/Set5/"

face_images = glob.glob('/content/drive/My Drive/TA SRCNN/Data/Set5/*.png') #gives path

print(face_images[:10], len(face_images))

with open('face_images_path.pickle','wb') as f: 
  pickle.dump(face_images,f)

from tqdm import tqdm         #A progess library
from multiprocessing import Pool
progress = tqdm(total= len(face_images), position=0)  

print(progress)
def read(path):
  img = image.load_img(path, target_size=(80,80,3))
  img = image.img_to_array(img)
  img = img/255.
  progress.update(1)
  return img

#p = Pool(1)
p = Pool(10) #awalnya ini
img_array = p.map(read, face_images)

ls

with open('img_array.pickle','wb') as f:
  pickle.dump(img_array, f)

len(img_array)

with open('img_array.pickle','rb') as f:
  img_array = pickle.load(f)

ls

plt.imshow(img_array[2])

all_images = np.array(img_array)

#print(all_images)
all_images.shape

#Split test and train data. all_images will be our output images
train_x, val_x = train_test_split(all_images, random_state = 32, test_size=0.2)

#now we will make input images by lowering resolution without changing the size
def pixalate_image(image, scale_percent = 40):
  width = int(image.shape[1] * scale_percent / 100)
  height = int(image.shape[0] * scale_percent / 100)
  dim = (width, height)

  small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)
  
  # scale back to original size
  width = int(small_image.shape[1] * 100 / scale_percent)
  height = int(small_image.shape[0] * 100 / scale_percent)
  dim = (width, height)

  low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)

  return low_res_image

train_x_px = []

for i in range(train_x.shape[0]):
  temp = pixalate_image(train_x[i,:,:,:])
  train_x_px.append(temp)

train_x_px = np.array(train_x_px)


# get low resolution images for the validation set
val_x_px = []

for i in range(val_x.shape[0]):
  temp = pixalate_image(val_x[i,:,:,:])
  val_x_px.append(temp)

val_x_px = np.array(val_x_px)

train_x_px.shape

val_x_px.shape

ls

plt.imshow(train_x[6])

plt.imshow(train_x_px[6])

# SRCNN original
Input_img = Input(shape=(80, 80, 3))  

x1 = Conv2D(64, (9, 9), strides=(1,1), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(Input_img)
x2 = Conv2D(32, (5, 5), strides=(1,1), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1)
x3 = Conv2D(3, (5, 5), strides=(1,1), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x2)

srcnn_model = Model(Input_img, x3)
srcnn_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])
srcnn_model.summary()

early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='min')
model_checkpoint = ModelCheckpoint('model_1',save_best_only=True)
abcd = srcnn_model.fit(train_x_px,train_x, epochs=50, validation_data=(val_x_px, val_x), callbacks=[early_stopper, model_checkpoint])
srcnn_model = tf.keras.models.load_model('model_1')
predictions = srcnn_model.predict(val_x_px)

n = 4
plt.figure(figsize= (20,10))

for i in range(n):
  ax = plt.subplot(3, n, i+1)
  ax.set_title("Distorted (Low Resolution) Image")
  plt.imshow(val_x_px[i+20])
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  ax = plt.subplot(3, n, i+1+n)
  ax.set_title("Predicted Image")
  plt.imshow(predictions[i+20])
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  ax = plt.subplot(3, n, i+1+n+n)
  ax.set_title("Original image")
  plt.imshow(val_x[i+20])
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

plt.show()

results = srcnn_model.evaluate(val_x_px, val_x)
print('loss, accuracy', results)

img = image.load_img('/content/drive/My Drive/TA SRCNN/Data/Set5/bird_0deg.png', target_size=(80,80,3))
img = image.img_to_array(img)
img = img/255.
img = pixalate_image(img)
plt.imshow(img)
# di save jadi download1.png

input_array = np.array([img])
#print(input_array)
predict = srcnn_model.predict(input_array)
#print(predict)
plt.imshow(predict[0])
# di save jadi download2.png

import math
from sklearn.metrics import mean_squared_error
from skimage.metrics import structural_similarity as ssim
from math import sqrt

diff = input_array - predict
mse = np.mean((diff) ** 2)
psnr = 20 * math.log10(255.0 / math.sqrt(mse))
rmse = math.sqrt(np.mean(diff ** 2.))
ssim = data_range=img.max() - img.min()

#psnr = 20 * math.log10(255. / rmse)

print('mse=')
print(mse)

print('rmse=')
print(rmse)

print('psnr=')
print(psnr)

print('SSIM')
print(ssim)